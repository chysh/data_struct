# 经典算法
## 排序算法
    最常用的一些排序算法算法：冒泡排序、插入排序、选择排序、快速排序、归并排序、基数排序、计数排序、桶排序。</br>
    按时间复杂度可以分为三类：</br>
      排序算法                时间复杂度         是否基于比较
      冒泡、插入、选择           O(n2)               Y
      快排、归并                O(nlogn)            Y
      桶、计数基数               O(n)                N
 
分析排序算法需从三方面考虑：</br>

1.排序算法的执行效率
    
    1.最好情况、最坏情况、平均情况时间复杂度
        在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度，
        此外还要说出最好、最坏时间复杂度对应的原始数据是什么样的。之所以要区分这三种时间复杂度原因有二：
        1.有些排序算法需要区分，为了好对比
        2.对于要排序的数据，有的接近有序，有的完全无序，有序度不同的数据，对排序的执行时间是有影响的，
        我们要知道排序算法在不同数据下的性能表现。
    2.时间复杂度的系数、常数、低阶
        时间复杂度反应的是数据规模n很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但在实际的软件开发中，
        我们排序的数据可能是10个、100个或者1000个这样的小规模数据，所以在对同一阶时间复杂的排序算法的性能对比的时候，
        就要把这些系数、常数、低阶也考虑进来。
    3.比较次数和交换（移动）次数
        基于比较的排序算法的执行过程中会涉及到两种操作，一种是元素比较大小，一种是元素移动或交换，所以我们在分析排序算法的
        执行效率的时候，应该把比较次数或交换（移动）次数也考虑进去。

2.排序算法的内存消耗

    算法的内存消耗可以通过空间复杂度来衡量，同样排序算法也不例外，此外针对排序算法还有一个概念叫原地排序（Sorted in place）。
    原地排序算法就是特指空间复杂度是O（1）的排序算法。
    
3.排序算法的稳定性
    
    仅用执行效率和内存消耗来衡量排序算法的还坏是不够的，针对排序算法还有一个重要的衡量指标，叫稳定性。意思是说待排序的序列中
    存在值相等的元素，经过排序之后，相等元素原有的前后顺序不变。

冒泡、插入、选择

    时间复杂度都是O（n2），适合小规模数据
    
归并、快排
    
    归并排序和快速排序的算法时间复杂度都是O(nlogn),适合大规模数据，较上三个算法更常用。
    这两种排序算法都运用了分治思想。分治，顾名思义就是分而治之，将一个大的问题分解成小的子问题，小问题解决了，大问题就解决了。
    分治思想与递归思想类似，其实分治算法就是通过递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，二者并不冲突。
归并排序原理：
    
    其核心思想是把要排序的数组从中间分成前后两部分，然后对前后两部分分别排序，再将排序好的两部分合并，这样整个数组就排序好了。
如图：
![image](https://github.com/chysh/data_struct/blob/master/images/guibing_20190926113654.jpg)

    归并排序用分之思想，可以用递归来实现。递归代码实现的技巧是：分析出递推公式，找到终止条件，最后将递推公式翻译成代码。
        递推公式：
        merge_sort(p…r) = merge(merge_sort(p…q),merge_sort(q+1…r）)
        终止条件：
        q >= r 不用再继续分解
    merge_sort(p…r)表示给下标从p到r的数组排序，将这个排序问题转换成两个子问题，merge_sort(p…q)和merge_sort(q+1…r），
    q是p和r的中间位置，也就是（p+r）/2，当下标从p到q和q+1到r这两个子数组排好序之后，再将这两个排好序的子数组合并在一起，
    这样下标p到r的数组就排好序了。
    代码实现见程序[merge_sort.c].
快速排序原理：

    其核心思想是把下标为p到r的一组数据排序，选择p到r之间的任意一个数据作为pivot（分去点）。然后遍历p到r的数据，
    将小于pivot数放在左边，大于pivot的数据放在右边，pivot放在中间，经过这一步骤之后，p到r之间的数据就被分成了三部分，
    其中前面p到q-1是小于pivot的部分，中间是pivot，后面的q+1到r的部分大于pivot。
![image](https://github.com/chysh/data_struct/blob/master/images/quick_sort_1.jpg)
    
    根据分治、递归的处理思想，可以用递归排序下标从p到q-1和q+1到r之间的数据，直到区间缩小为1，则说明所有的数据都有序了。
        递推公式：
        quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1…r)
        终止条件：
        p >= r
分区过程如下可以实现原地排序：（一般可以选择p到r之间的最后一个元素作为pivot）
![image](https://github.com/chysh/data_struct/blob/master/images/pivot.jpg)
    
    代码实现见程序[quick_sort.c]
    
  归并与快排区别：
    
    1.归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。
    理解归并排序的重点是理解递推公式和 merge() 合并函数。同理，理解快排的重点也是理解递推公式，还有 partition() 分区函数。
    2.归并排序算法是一种在任何情况下时间复杂度都比较稳定的排序算法，这也使它存在致命的缺点，即归并排序不是原地排序算法，
    空间复杂度比较高，是 O(n)。正因为此，它也没有快排应用广泛。
    3.快速排序算法虽然最坏情况下的时间复杂度是 O(n2)，但是平均情况下时间复杂度都是 O(nlogn)。
    不仅如此，快速排序算法时间复杂度退化到 O(n2) 的概率非常小，我们可以通过合理地选择 pivot 来避免这种情况。

线性排序
    
    时间复杂度为O（n）的排序称为线性排序，包括桶排序、基数排序、计数排序。之所以能做到线性的时间复杂度，是因为这三个算法是
    非基于比较的排序算法，都不涉及元素之间的比较。
    
桶排序（Bucket sort）

    桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，
    再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了
![image](https://github.com/chysh/data_struct/blob/master/images/bucket_20190927093708.jpg)

    桶排序复杂度为什么是O(n),分析如下：
    如果要排序的数据有 n 个，把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为
    O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。
    当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。
    
    【桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？】：
        答案当然是否定的。实际上，桶排序对要排序数据的要求是非常苛刻的。
        首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，
        桶与桶之间的数据不需要再进行排序。
        其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，
        那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。
        
     【桶排序比较适合用在外部排序中】。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存。
      比如说我们有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，
      没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？
      这就可以借助桶排序的处理思想来解决这个问题。首先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，
      订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1元到 1000 元
      之内的订单，第二桶存储金额在 1001元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号
      命名（00，01，02…99）。
      理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约100MB 的订单
      数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次
      读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。
      不过，有可能订单按照金额在 1 元到 10 万元之间并不一定是均匀分布的 ，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。
      有可能某个金额区间的数据特别多划分之后对应的文件就会很大，没法一次性读入内存。这又该怎么办呢？
      针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为
      10 个小区间，1 元到 100 元，101元到 200 元，201 元到 300 元…901 元到 1000 元。如果划分之后，101 元到 200 元之间的订单
      还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。
    
----------------------------------------------------------------------------------------------------------------------------

二分查找
    
